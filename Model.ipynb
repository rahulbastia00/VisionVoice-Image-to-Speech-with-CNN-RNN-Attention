{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30171,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:170%;text-align:center;border-radius:20px 60px;\">EYE FOR BLIND</p>\n\n![](https://images.immediate.co.uk/production/volatile/sites/4/2018/08/iStock_50575182_LARGE-b054f52-e1533553873292.jpg?quality=90&resize=768,574)\n\n**Please Note :** *Google Colab has been used to train the model*\n\n\n# <p style=\"background-color:white;font-family:seoge print;color:gray ;font-size:170%;text-align:center;border-radius:20px 60px;\">INTRODUCTION</p>\n\n\n\n#### The World Health Organization (WHO) has reported that approximately 285 million people are visually impaired worldwide, and out of these 285 million, 39 million are completely blind. It gets extremely tough for them to carry out daily activities, one of which is reading. \n\n    From reading a newspaper or a magazine to reading an important text message from your bank, it is tough for them to read the text written in it.A similar problem they also face is seeing and enjoying the beauty of pictures and images. Today, in the world of social media, millions of images are uploaded daily. Some of them are about your friends and family, while some of them are about nature and its beauty. Understanding what is present in that image is quite a challenge for certain people who are suffering from visual impairment or who are blind.\n\n\n*In an initiative to help them experience the beauty of the images, Facebook had earlier launched a unique feature earlier that can help blind people operate the app on their mobile phones. The feature could explain the contents of an image that their friends have posted on Facebook. So, say, if someone posted a picture with their dog in the park, the application would speak out the contents and may describe it like, “This image may contain a dog standing with a man around the trees.”*\n\n**Here, in this kernel will learn how to make a model that is similar to the one developed by Facebook, specifically such that a blind person knows the contents of an image in front of them with the help of a CNN-RNN based model. The model will convert the contents of an image and will provide the output in the form of audio.**\n\n\n\n    In this project, we will convert an image to text description first; then, using a simple text-to-speech API, we will extract the text description/caption and convert it to audio. So the central part is focused on building the caption/text description whereas the second part, which is transforming the text to speech is relatively easy with the text to speech API. Once the model is built, we will deploy the project on our local system using a Flask-based model to generate audio-based content for any image.\n    \n    You will learn how to make a model that is similar to the one developed by Facebook, specifically such that a blind person knows the contents of an image in front of them with the help of a CNN-RNN based model. The model will convert the contents of an image and will provide the output in the form of audio.\n   \n   \n # <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:130%;text-align:center;border-radius:20px 60px;\">Let's get started . . .</p>","metadata":{"id":"uqDujPax5GBJ"}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">CONTENTS</p>\n----------------------------------------------------------------------------------------------------------\n\n\n**1. Initial Setup**\n\n        Import all required libraries\n        \n        Install additional libraries (if required)\n        \n        Mount drive and import dataset\n\n**2. Data Understanding and Visualization**\n\n        Unzip data files and read it's content\n        \n        Save the images and captions in two different folders (and variables)\n        \n        Visualize Images and Captions in the dataset\n        \n        Create dataframe to store images along with thir respective captions and path\n        \n        Visualize the top 30 most occurring words in the captions (through bar plots and wordcloud map)\n        \n\n**3. Data Cleaning**\n\n        Remove punctuations\n        \n        Convert captions to lowercase\n        \n        Remove numeric values and retain words in captions\n        \n        \n**4. Data Preprocessing**\n \n   > **Captions Preprocessing**\n \n         Create tokenizer (remove spaces and punctuation and retain unique words)\n         \n         Replace words beyond count of 5000 with tag : UNK\n         \n         Create word-to-index and index-to-word mappings\n        \n         Pad sequences to be of same length (as that of the longest)\n         \n         Finally view index with words and tokenizers\n         \n         \n   > **Images Preprocessing**\n   \n          Resize images to shape : (299, 299)\n           \n          Normalize the image within the range of -1 to 1 (desired format for InceptionV3)\n           \n         \n**5. Dataset Creation**\n\n        1. Apply train_test_split on both image path & captions to create the train & test list. \n\n        2. Create a function which maps the image path to their feature.\n\n        3. Create a builder function to create train & test dataset & transform the dataset\n        \n        4. Load the pretrained Imagenet weights of Inception net V3\n\n        5. Shuffle and batch while building the dataset\n\n        6. Ensure shape of each image in the dataset : (batch_size, 8*8, 2048)\n\n        7. Ensureshape of each caption in the dataset : (batch_size, max_len)\n        \n\n**6. Model Building**\n\n        Set model parameters (units, embed_dim, vocab_size, train & test num steps, feature_shape, max_length)\n        \n        Build Encoder\n        \n        Build Attention Model\n         \n        Build Decoder\n        \n        \n**7. Model Training and Optimization**\n\n        1.Set the optimizer & loss object\n\n        2.Create your checkpoint path\n\n        3.Create your training & testing step functions\n\n        4.Create your loss function for the test dataset\n        \n**8. Model Evaluation**\n\n        Set up evaluation function using Greedy Search\n        \n        Test on sample data images\n        \n        Evaluate on the basis of BLEU Score\n        \n        ","metadata":{"id":"1Ot3idpMoqVy"}},{"cell_type":"markdown","source":"\n# <p style=\"background-color:white;font-family:arial;font-size:150%;color:darkslateblue;text-align:center;border-radius:20px 60px;\">Objective </p>\n\n > **Create a deep learning model which can explain the contents of an image in the form of speech through caption generation with an attention mechanism on Flickr8K dataset.**\n \n >**This kind of model is a use-case for blind people so that they can understand any image with the help of speech.** The caption generated through a CNN-RNN model will be converted to speech using a text to speech library. **The model will convert the contents of an image and will provide the output in the form of audio.**","metadata":{"id":"UcBuxWq7oqV3"}},{"cell_type":"markdown","source":"## Techniques Used \n\n    1. Traditional CNN-RNN model\n\n    2. Attention model\n\n    3. Text and Image preprocessing\n\n    4. Data Visualization using wordcloud\n\n    5. Model training and model evaluation (INCEPTIONV3)\n\n    6. Greedy search vs Beam search and BLUE Score\n    \n","metadata":{"id":"pGTvNKGmoqV6"}},{"cell_type":"markdown","source":"# Let's Dive in . . .\n--------------------------------------------","metadata":{"id":"cABLPTsJoqV7"}},{"cell_type":"markdown","source":"\n<a id=\"1\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Initial Setup</p>","metadata":{"id":"Y7s-FNSm86zQ"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"5VEkNLugWxE_","outputId":"0ca2acb2-9029-4d97-af8d-85991ee6f519","scrolled":true,"execution":{"iopub.status.busy":"2022-05-01T11:11:10.131634Z","iopub.execute_input":"2022-05-01T11:11:10.132247Z","iopub.status.idle":"2022-05-01T11:11:10.850832Z","shell.execute_reply.started":"2022-05-01T11:11:10.132165Z","shell.execute_reply":"2022-05-01T11:11:10.850051Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.1 Import all the required libraries","metadata":{"id":"at2VLWk5oqWB"}},{"cell_type":"code","source":"#Import all the required libraries\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\nimport tensorflow as tf\nimport keras\nfrom keras.preprocessing.image import load_img\nimport string\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import Input\nfrom PIL import Image\n\n#used for creating Progress Meters or Progress Bars\nfrom tqdm import tqdm\n","metadata":{"id":"Km6gyfnn5GBX","execution":{"iopub.status.busy":"2022-05-01T11:11:10.853078Z","iopub.execute_input":"2022-05-01T11:11:10.853345Z","iopub.status.idle":"2022-05-01T11:11:16.604324Z","shell.execute_reply.started":"2022-05-01T11:11:10.853308Z","shell.execute_reply":"2022-05-01T11:11:16.603568Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2 Install additional libraries required","metadata":{"id":"sfyVqvE-oqWG"}},{"cell_type":"markdown","source":"We need to install the below libraries before proceeding :\n\n**gTTS** - (Google Text-to-Speech)is a Python library and CLI tool to interface with Google Translate text-to-speech API. We will import the gTTS library from the gtts module which can be used for speech translation.\n\n**playsound** - The playsound module is a cross platform module that can play audio files. This doesn't have any dependencies, simply install with pip in your virtualenv and run!\n\n**display** - Public API for display tools in IPython","metadata":{"id":"AwfD_GzD32ia"}},{"cell_type":"code","source":"# Install additional libraries required\n\n!pip install wordcloud\n!pip install gtts\n!pip install playsound","metadata":{"id":"hlMumuYS32Qq","outputId":"637d8e73-3355-4bf4-d51e-5008245d691d","execution":{"iopub.status.busy":"2022-05-01T11:11:16.607533Z","iopub.execute_input":"2022-05-01T11:11:16.607757Z","iopub.status.idle":"2022-05-01T11:11:42.598948Z","shell.execute_reply.started":"2022-05-01T11:11:16.607729Z","shell.execute_reply":"2022-05-01T11:11:42.598145Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nfrom gtts import gTTS\nfrom playsound import playsound\nfrom IPython import display\n\nimport collections\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\n","metadata":{"id":"MUXkkyzu3qNd","outputId":"c8bd3b31-bcbc-4eaf-f31b-f29b144d2386","execution":{"iopub.status.busy":"2022-05-01T11:11:42.601883Z","iopub.execute_input":"2022-05-01T11:11:42.60216Z","iopub.status.idle":"2022-05-01T11:11:42.651187Z","shell.execute_reply.started":"2022-05-01T11:11:42.602123Z","shell.execute_reply":"2022-05-01T11:11:42.650505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.3 Mount drive and import dataset","metadata":{"id":"kYiFTZgroqWL"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"-TJLldZcKn-x","outputId":"062035f8-901b-42c3-8103-d73a7294a514","execution":{"iopub.status.busy":"2022-05-01T11:11:42.652271Z","iopub.execute_input":"2022-05-01T11:11:42.653067Z","iopub.status.idle":"2022-05-01T11:11:48.744463Z","shell.execute_reply.started":"2022-05-01T11:11:42.653029Z","shell.execute_reply":"2022-05-01T11:11:48.743567Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<a id=\"1\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Let's Understand our Data</p>","metadata":{"id":"mIfI_sVA5GBb"}},{"cell_type":"markdown","source":"## 2. Data Understanding and Visualization\n\n1.Import the dataset and read image & captions into two seperate variables\n\n2.Visualise both the images & text present in the dataset\n\n3.Create a dataframe which summarizes the image, path & captions as a dataframe\n\n4.Create a list which contains all the captions & path\n\n5.Visualise the top 30 occuring words in the captions\n\n","metadata":{"id":"llXKfPgc5GBc"}},{"cell_type":"markdown","source":"### 2.1 Import the dataset and read image & captions into two separate variables","metadata":{"id":"G_oy951S9lnS"}},{"cell_type":"code","source":"#Import the dataset and read the image into a seperate variable\n\nimages='/kaggle/input/flickr8k/Images'\n\nall_imgs = glob.glob(images + '/*.jpg',recursive=True)\nprint(\"The total images present in the dataset: {}\".format(len(all_imgs)))","metadata":{"id":"_HQhjkDx5GBe","outputId":"1062e884-1913-4e02-da0b-3e342f0a2a25","execution":{"iopub.status.busy":"2022-05-01T11:11:48.747034Z","iopub.execute_input":"2022-05-01T11:11:48.747314Z","iopub.status.idle":"2022-05-01T11:11:48.872962Z","shell.execute_reply.started":"2022-05-01T11:11:48.747277Z","shell.execute_reply":"2022-05-01T11:11:48.872421Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Visualise both the images & text present in the dataset","metadata":{"id":"QjEDFNHA9o_4"}},{"cell_type":"code","source":"#Visualise both the images & text present in the dataset\nimport imageio\n\n#Visualising first 5 images :\n\nDisplay_Images = all_imgs[0:5]\nfigure, axes = plt.subplots(1,5)\nfigure.set_figwidth(20)\n\nfor ax, image in zip(axes, Display_Images):\n  ax.imshow(imageio.imread(image), cmap=None)\n","metadata":{"id":"-5JgWYQO5GBf","outputId":"81488627-15bb-4589-96ec-793d512d5586","execution":{"iopub.status.busy":"2022-05-01T11:11:48.873914Z","iopub.execute_input":"2022-05-01T11:11:48.874149Z","iopub.status.idle":"2022-05-01T11:11:49.779043Z","shell.execute_reply.started":"2022-05-01T11:11:48.874112Z","shell.execute_reply":"2022-05-01T11:11:49.778353Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# view a random image\n\nimport random\nImage.open(all_imgs[random.randrange(40, 60, 3)])","metadata":{"id":"T266bICHP8YN","outputId":"fcc79f82-24e0-4643-f8a7-b117264d2dc8","execution":{"iopub.status.busy":"2022-05-01T11:11:49.780276Z","iopub.execute_input":"2022-05-01T11:11:49.780698Z","iopub.status.idle":"2022-05-01T11:11:49.852914Z","shell.execute_reply.started":"2022-05-01T11:11:49.780649Z","shell.execute_reply":"2022-05-01T11:11:49.852119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Import the dataset and read the text file into a separate variable\n\n#txt_file = 'Flickr8K/captions.txt'\ntext_file = '/kaggle/input/flickr8k/captions.txt'\n\ndef load_doc(filename):\n    \n    #your code here\n    open_file = open(text_file, 'r', encoding='latin-1' ) #returns a file object\n    text = open_file.read() #reads contents of the file\n    open_file.close()\n    #print(text)\n    \n    return text\n\ndoc = load_doc(text_file)\nprint(doc[:300])","metadata":{"id":"P_XPOhV75GBh","outputId":"a3d73567-6d39-4af1-e6e0-8d9136ec4c7e","scrolled":true,"execution":{"iopub.status.busy":"2022-05-01T11:11:49.854316Z","iopub.execute_input":"2022-05-01T11:11:49.854556Z","iopub.status.idle":"2022-05-01T11:11:49.894167Z","shell.execute_reply.started":"2022-05-01T11:11:49.854525Z","shell.execute_reply":"2022-05-01T11:11:49.89334Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Create a dataframe which summarizes the image, path & captions as a dataframe","metadata":{"id":"nhQQ5bzHoqWX"}},{"cell_type":"markdown","source":"    Create a dataframe which summarizes the image, path & captions as a dataframe\n\n    Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples.","metadata":{"id":"QPE2vHNIoqWX"}},{"cell_type":"markdown","source":"### 2.3 Create a list which contains all the captions & path\n\n#### We will create three lists here and finally combine them to form a dataframe\n\n    1. all_img_id = [] to store all the image id \n    2. all_img_vector = [] to store all the image path \n    3. annotations = []  to store all the captions ","metadata":{"id":"V9yj_C0R-XlW"}},{"cell_type":"code","source":"img_path = '/kaggle/input/flickr8k/Images/'\n\nall_img_id = [] #store all the image id here\nall_img_vector = [] #store all the image path here\nannotations = [] #store all the captions here\n\nwith open('/kaggle/input/flickr8k/captions.txt' , 'r') as fo:\n  next(fo) #to skip the heading\n  for line in fo :\n    split_arr = line.split(',')\n    all_img_id.append(split_arr[0])\n    annotations.append(split_arr[1].rstrip('\\n.')) #removing out the \\n.\n    all_img_vector.append(img_path+split_arr[0])\n\ndf = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n    \ndf","metadata":{"id":"tqGD4H6S5GBk","outputId":"f3c214be-20ab-4aaf-ff0e-b8ca2b9a1b3e","execution":{"iopub.status.busy":"2022-05-01T11:11:49.897206Z","iopub.execute_input":"2022-05-01T11:11:49.897391Z","iopub.status.idle":"2022-05-01T11:11:49.986294Z","shell.execute_reply.started":"2022-05-01T11:11:49.897368Z","shell.execute_reply":"2022-05-01T11:11:49.985564Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len (annotations)","metadata":{"id":"Kj_280Yf9Zkg","outputId":"549031b1-8513-4ddb-a79e-e64257f14eba","execution":{"iopub.status.busy":"2022-05-01T11:11:49.987532Z","iopub.execute_input":"2022-05-01T11:11:49.987944Z","iopub.status.idle":"2022-05-01T11:11:49.993758Z","shell.execute_reply.started":"2022-05-01T11:11:49.987906Z","shell.execute_reply":"2022-05-01T11:11:49.992878Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type (annotations)","metadata":{"id":"I1TbYSa-WalN","outputId":"25ddf1df-8a37-4598-c1f7-703fd413a404","execution":{"iopub.status.busy":"2022-05-01T11:11:49.995221Z","iopub.execute_input":"2022-05-01T11:11:49.995605Z","iopub.status.idle":"2022-05-01T11:11:50.004379Z","shell.execute_reply.started":"2022-05-01T11:11:49.995568Z","shell.execute_reply":"2022-05-01T11:11:50.003454Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### So now we have successfully created a list containing all captions as well as a dataframe which contains all images along withh thir respective paths and captions\n\n   *Each image has 5 captions associated with it*\n    \n    Hence, total images : 8091\n    \n    Therefore, total captions : 8091*5 = 40455\n    \n**Let's verify this . .**    ","metadata":{"id":"l45k0QiZoqWc"}},{"cell_type":"code","source":"#check total captions and images present in dataset\n\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(all_imgs)))","metadata":{"id":"v_gaypCD5GBm","outputId":"5fd8920d-45f2-424f-a41b-3317becdb137","execution":{"iopub.status.busy":"2022-05-01T11:11:50.006059Z","iopub.execute_input":"2022-05-01T11:11:50.006646Z","iopub.status.idle":"2022-05-01T11:11:50.012689Z","shell.execute_reply.started":"2022-05-01T11:11:50.006607Z","shell.execute_reply":"2022-05-01T11:11:50.011881Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Create the vocabulary & the counter for the captions\n#lower() used to ensure same count irrespective of an alphabet's case\n\nvocabulary = [word.lower() for line in annotations for word in line.split()]\n\nval_count = Counter(vocabulary) \nval_count","metadata":{"id":"zvazRH035GBn","outputId":"08d44820-2bfa-4ab0-abc7-4baf133348f1","execution":{"iopub.status.busy":"2022-05-01T11:11:50.01433Z","iopub.execute_input":"2022-05-01T11:11:50.014965Z","iopub.status.idle":"2022-05-01T11:11:50.177557Z","shell.execute_reply.started":"2022-05-01T11:11:50.014925Z","shell.execute_reply":"2022-05-01T11:11:50.176903Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.5 Visualise the top 30 occuring words in the captions","metadata":{"id":"dIx0I0YV-iq5"}},{"cell_type":"code","source":"#Visualise the top 30 occuring words in the captions\n\n#write your code here\nfor word, count in val_count.most_common(30):\n  print(word, \": \", count)\n\nlst = val_count.most_common(30)\nmost_common_words_df = pd.DataFrame(lst, columns = ['Word', 'Count'])\nmost_common_words_df.plot.bar(x='Word', y='Count', width=0.6, color='orange', figsize=(15, 10))\nplt.title(\"Top 30 maximum frequency words\", fontsize = 18, color= 'navy')\nplt.xlabel(\"Words\", fontsize = 14, color= 'navy')\nplt.ylabel(\"Count\", fontsize = 14, color= 'navy')\n","metadata":{"id":"KUeQzrK_5GBo","outputId":"ec16ed25-89b4-4fcf-a964-df049c299b66","execution":{"iopub.status.busy":"2022-05-01T11:11:50.178549Z","iopub.execute_input":"2022-05-01T11:11:50.179099Z","iopub.status.idle":"2022-05-01T11:11:50.627151Z","shell.execute_reply.started":"2022-05-01T11:11:50.17906Z","shell.execute_reply":"2022-05-01T11:11:50.626489Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Clearly STOPWORDS form a clear majority\n\n**Stop words are a set of commonly used words in a language.**\n\n    Examples of stop words in English are “a”, “the”, “is”, “are” and etc. \n    \n*Stop words are commonly used in **Text Mining and Natural Language Processing (NLP)** to eliminate words that are so commonly used that they carry very little useful information.*","metadata":{"id":"IcjI9XFaoqWe"}},{"cell_type":"code","source":"## check out the top 30 stopwords with higher frequency\n\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(val_count)\nplt.figure(figsize = (12, 12))\nplt.imshow(wordcloud)","metadata":{"id":"eNuz3uTj-5Bx","outputId":"c421293f-805c-421e-8909-1152ded2120d","execution":{"iopub.status.busy":"2022-05-01T11:11:50.628525Z","iopub.execute_input":"2022-05-01T11:11:50.628986Z","iopub.status.idle":"2022-05-01T11:11:51.883284Z","shell.execute_reply.started":"2022-05-01T11:11:50.628947Z","shell.execute_reply":"2022-05-01T11:11:51.882546Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize the images and captions together","metadata":{"id":"jjgWEUPy_ySJ"}},{"cell_type":"code","source":"def caption_with_img_plot(image_id, frame) :\n  #get the captions\n  capt = (\"\\n\" *2).join(frame[frame['ID'] == image_id].Captions.to_list())\n  fig, ax = plt.subplots()\n  ax.set_axis_off()\n  idx = df.ID.to_list().index(image_id)\n  im =  Image.open(df.Path.iloc[idx])\n  w, h = im.size[0], im.size[-1]\n  ax.imshow(im)\n  ax.text(w+50, h, capt, fontsize = 18, color = 'navy')\ncaption_with_img_plot(df.ID.iloc[8049], df)","metadata":{"id":"IyZhuUyG_0xN","outputId":"438d29a4-936c-44de-eacf-fe965dce9c19","scrolled":true,"execution":{"iopub.status.busy":"2022-05-01T11:11:51.884484Z","iopub.execute_input":"2022-05-01T11:11:51.88516Z","iopub.status.idle":"2022-05-01T11:11:52.129176Z","shell.execute_reply.started":"2022-05-01T11:11:51.88512Z","shell.execute_reply":"2022-05-01T11:11:52.128506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def execute_img_capt(start, end, frame) :\n  for r in range(start, end) :\n    caption_with_img_plot(frame.ID.drop_duplicates().iloc[r], frame)\n\nexecute_img_capt(0, 5, df)  ","metadata":{"id":"deUxtBv0CY4J","outputId":"085d83ca-f108-4206-fefc-31dd75b74239","execution":{"iopub.status.busy":"2022-05-01T11:11:52.130396Z","iopub.execute_input":"2022-05-01T11:11:52.13087Z","iopub.status.idle":"2022-05-01T11:11:53.231143Z","shell.execute_reply.started":"2022-05-01T11:11:52.130831Z","shell.execute_reply":"2022-05-01T11:11:53.230501Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Clean Data</p>\n\n1. Remove punctuations\n2. Convert captions to lowercase\n3. Retain words and eliminate numeric values","metadata":{"id":"OWCnC35pimM5"}},{"cell_type":"code","source":"#data cleaning\nrem_punct = str.maketrans('', '', string.punctuation)\nfor r in range(len(annotations)) :\n  line = annotations[r]\n  line = line.split()\n\n  # converting to lowercase\n  line = [word.lower() for word in line]\n\n  # remove punctuation from each caption and hanging letters\n  line = [word.translate(rem_punct) for word in line]\n  line = [word for word in line if len(word) > 1]\n\n  # remove numeric values\n  line = [word for word in line if word.isalpha()]\n\n  annotations[r] = ' '.join(line)\n","metadata":{"id":"GSyzTXMUjJsP","execution":{"iopub.status.busy":"2022-05-01T11:11:53.232292Z","iopub.execute_input":"2022-05-01T11:11:53.232965Z","iopub.status.idle":"2022-05-01T11:11:53.659858Z","shell.execute_reply.started":"2022-05-01T11:11:53.232925Z","shell.execute_reply":"2022-05-01T11:11:53.659106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#add the <start> & <end> token to all those captions as well\nannotations = ['<start>' + ' ' + line + ' ' + '<end>' for line in annotations]\n\n#Create a list which contains all the path to the images\nall_img_path = all_img_vector","metadata":{"id":"SPQBB9WNikB3","execution":{"iopub.status.busy":"2022-05-01T11:11:53.66117Z","iopub.execute_input":"2022-05-01T11:11:53.661415Z","iopub.status.idle":"2022-05-01T11:11:53.680251Z","shell.execute_reply.started":"2022-05-01T11:11:53.661381Z","shell.execute_reply":"2022-05-01T11:11:53.679589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##list contatining captions for an image\nannotations[0:5]","metadata":{"id":"iT3gHUXrikNR","outputId":"8a7b53aa-ab7b-4734-a5c4-a0863a17af6a","execution":{"iopub.status.busy":"2022-05-01T11:11:53.681417Z","iopub.execute_input":"2022-05-01T11:11:53.681677Z","iopub.status.idle":"2022-05-01T11:11:53.687247Z","shell.execute_reply.started":"2022-05-01T11:11:53.681637Z","shell.execute_reply":"2022-05-01T11:11:53.686486Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Data Preprocessing</p>\n\n\n### 4.1 Captions Preprocessing\n\n\nPre-Processing the captions\n\n1. Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \nThis gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n\n2. Replace all other words with the unknown token \"UNK\" .\n\n3. Create word-to-index and index-to-word mappings.\n\n4. Pad all sequences to be the same length as the longest one.","metadata":{"id":"8RYtXmZc5GBo"}},{"cell_type":"code","source":"# create the tokenizer\n\n#your code here\n\ntop_word_cnt = 5000\ntokenizer = Tokenizer(num_words = top_word_cnt+1, filters= '!\"#$%^&*()_+.,:;-?/~`{}[]|\\=@ ',\n                      lower = True, char_level = False, \n                      oov_token = 'UNK')","metadata":{"id":"FXlFvhTS5GBp","execution":{"iopub.status.busy":"2022-05-01T11:11:53.68864Z","iopub.execute_input":"2022-05-01T11:11:53.689635Z","iopub.status.idle":"2022-05-01T11:11:53.695599Z","shell.execute_reply.started":"2022-05-01T11:11:53.689552Z","shell.execute_reply":"2022-05-01T11:11:53.694731Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create word-to-index and index-to-word mappings.\n\n#your code here\n\ntokenizer.fit_on_texts(annotations)\n\n#transform each text into a sequence of integers\ntrain_seqs = tokenizer.texts_to_sequences(annotations)","metadata":{"id":"zWSduzQ75GBp","execution":{"iopub.status.busy":"2022-05-01T11:11:53.696884Z","iopub.execute_input":"2022-05-01T11:11:53.697737Z","iopub.status.idle":"2022-05-01T11:11:54.985514Z","shell.execute_reply.started":"2022-05-01T11:11:53.697697Z","shell.execute_reply":"2022-05-01T11:11:54.984642Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Why Padding ?\n\n > Each individual image has **five captions which are all of different lengths**. Hence they **can't be fed directly to the Decoder.**\n    \n > We will use **PADDING, to bring all captions to a fixed length** before proceeding further.\n\n > Padding is a process that we apply at the **start or the end of a sequence to make all the samples to have a common standard length.** \n    \n > For text generation, the **decoder needs to have an input at the start and it can't be a padded input.**\n    \n > Therefore, for our process, we will be doing the **padding at the end** (of the caption sequence).\n\n > All **padded values will be masked and set to 'False' while the rest will be set to 'True'**\n    \n > **All True values are assigned the value : 1, while padded, i.e False values are set to 0**\n    \n    \n*Padding can result in a risk of adding penalty to the model.* \n\n*We apply masking to rectify the same and this will truncate down all the added penalties back to zero :*\n\n    \n### Loss  = Loss * Mask","metadata":{}},{"cell_type":"markdown","source":"As we will use pad_sequence to process the sequence, we will use 0 as the padding value.\n\n > Keras uses a different index for UNKNOWN, in order to distinguish between PAD and UNKNOWN\n\n  > 0 is the reserved index which has no word assigned.","metadata":{"id":"yPtajgEkozD3"}},{"cell_type":"code","source":"# we add PAD token for zero\n\ntokenizer.word_index['PAD'] = 0\ntokenizer.index_word[0] = 'PAD'","metadata":{"id":"PtcoGgGRowXe","execution":{"iopub.status.busy":"2022-05-01T11:11:54.987036Z","iopub.execute_input":"2022-05-01T11:11:54.987285Z","iopub.status.idle":"2022-05-01T11:11:54.992565Z","shell.execute_reply.started":"2022-05-01T11:11:54.987251Z","shell.execute_reply":"2022-05-01T11:11:54.991198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.oov_token)\nprint(tokenizer.index_word[0])","metadata":{"id":"45n08QRYphY9","outputId":"2564e83c-0fb8-4334-f241-52b41315f860","execution":{"iopub.status.busy":"2022-05-01T11:11:54.994296Z","iopub.execute_input":"2022-05-01T11:11:54.994738Z","iopub.status.idle":"2022-05-01T11:11:55.003239Z","shell.execute_reply.started":"2022-05-01T11:11:54.994678Z","shell.execute_reply":"2022-05-01T11:11:55.002189Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### View index words","metadata":{"id":"l8ABdpoqpj6a"}},{"cell_type":"code","source":"tokenizer.index_word","metadata":{"id":"k453YjB5piwH","outputId":"a606833c-a1e1-423f-a353-6c0b6e0347e1","scrolled":true,"execution":{"iopub.status.busy":"2022-05-01T11:11:55.005276Z","iopub.execute_input":"2022-05-01T11:11:55.005962Z","iopub.status.idle":"2022-05-01T11:11:55.045472Z","shell.execute_reply.started":"2022-05-01T11:11:55.00592Z","shell.execute_reply":"2022-05-01T11:11:55.044707Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a word count of your tokenizer to visualize the Top 30 occuring words after text processing\n\n#your code here\ntokenizer_top_words = [word for line in annotations for word in line.split() ]\n\n#tokenizer_top_words_count\ntokenizer_top_words_count = collections.Counter(tokenizer_top_words)\ntokenizer_top_words_count","metadata":{"id":"UhXgkeld5GBp","outputId":"bd5c8d71-46c4-42b5-9df3-fba5583a409e","execution":{"iopub.status.busy":"2022-05-01T11:11:55.046897Z","iopub.execute_input":"2022-05-01T11:11:55.047471Z","iopub.status.idle":"2022-05-01T11:11:55.190569Z","shell.execute_reply.started":"2022-05-01T11:11:55.047435Z","shell.execute_reply":"2022-05-01T11:11:55.189679Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for word, count in tokenizer_top_words_count.most_common(30) :\n  print(word, \": \", count)\n\ntokens = tokenizer_top_words_count.most_common(30)\nmost_com_words_df = pd.DataFrame(tokens, columns = ['Word', 'Count'])\n\n#plot 30 most common words\nmost_common_words_df.plot.bar(x = 'Word', y= 'Count', width=0.8, color = 'indigo', figsize = (17, 10))\nplt.title('Top 30 common words', fontsize =20, color= 'navy')\nplt.xlabel('Words', fontsize =14, color= 'navy')\nplt.ylabel('Counts', fontsize =14, color= 'navy')\nplt.grid(b=None)","metadata":{"id":"TDnbKRVdrX8u","outputId":"d76adf4f-b190-4c80-9cdf-03aa8637ed13","execution":{"iopub.status.busy":"2022-05-01T11:11:55.202644Z","iopub.execute_input":"2022-05-01T11:11:55.202886Z","iopub.status.idle":"2022-05-01T11:11:55.688416Z","shell.execute_reply.started":"2022-05-01T11:11:55.20285Z","shell.execute_reply":"2022-05-01T11:11:55.687518Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Let's view the frequency of words now, post Captions preprocessing","metadata":{"id":"WQY_6HARoqWm"}},{"cell_type":"code","source":"wordcloud_token = WordCloud(width = 1000, height = 500).generate_from_frequencies(tokenizer_top_words_count)\nplt.figure(figsize = (12, 8))\nplt.imshow(wordcloud_token)\nplt.grid(b = None)","metadata":{"id":"mQC1nDSPtlig","outputId":"42373cb9-76ee-4947-b27b-371e938ca1bd","execution":{"iopub.status.busy":"2022-05-01T11:11:55.691481Z","iopub.execute_input":"2022-05-01T11:11:55.691705Z","iopub.status.idle":"2022-05-01T11:11:57.088152Z","shell.execute_reply.started":"2022-05-01T11:11:55.691662Z","shell.execute_reply":"2022-05-01T11:11:57.087346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pad each vector to the max_length of the captions ^ store it to a vairable\n\n#store the length of all lists\ntrain_seqs_len = [len(seq) for seq in train_seqs]\n\n#store elements from list with maximum value\nlongest_word_length = max(train_seqs_len)\n\n#calculate longest word_length and pads all sequences to equal length as that of the longest.\ncap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding= 'post', maxlen = longest_word_length,\n                                                          dtype='int32', value=0)\n\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))","metadata":{"id":"O-VPrP2z5GBq","outputId":"37690f40-5580-4434-9d27-c29d551909fd","execution":{"iopub.status.busy":"2022-05-01T11:11:57.089658Z","iopub.execute_input":"2022-05-01T11:11:57.089932Z","iopub.status.idle":"2022-05-01T11:11:57.2912Z","shell.execute_reply.started":"2022-05-01T11:11:57.089896Z","shell.execute_reply":"2022-05-01T11:11:57.290508Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**NOTE :** Using padding = 'post', we are padding zero at last. Starting word always acts as a crucial prompter and another word is expected after that. Zero at the beginning would indicate no value.\n\nHence in order to maintain existing sequence and it's meaning, we **apply PADDING at the end.**","metadata":{"id":"oNHqhwflvvDb"}},{"cell_type":"markdown","source":"## 4.2 Pre-processing the images\n\n1.Resize them into the shape of (299, 299)\n\n2.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. ","metadata":{"id":"7OW8tbjd5GBr"}},{"cell_type":"markdown","source":"### FAQs on how to resize the images::\n* Since you have a list which contains all the image path, you need to first convert them to a dataset using <i>tf.data.Dataset.from_tensor_slices</i>. Once you have created a dataset consisting of image paths, you need to apply a function to the dataset which will apply the necessary preprocessing to each image. \n* This function should resize them and also should do the necessary preprocessing that it is in correct format for InceptionV3.\n","metadata":{"id":"ufb7sKo95GBr"}},{"cell_type":"code","source":"# creating list to store preprocessed images and setting up the Image Shape\n\npreprocessed_image = []\nIMAGE_SHAPE = (299, 299)","metadata":{"id":"6QP1BmXP-e7W","execution":{"iopub.status.busy":"2022-05-01T11:11:57.292545Z","iopub.execute_input":"2022-05-01T11:11:57.293003Z","iopub.status.idle":"2022-05-01T11:11:57.297184Z","shell.execute_reply.started":"2022-05-01T11:11:57.292964Z","shell.execute_reply":"2022-05-01T11:11:57.296382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#checking image format \n\ntf.keras.backend.image_data_format()\n","metadata":{"id":"eq3ZEfqi5GBs","outputId":"7936374d-aa21-41de-cf73-18a3ebfaa50b","execution":{"iopub.status.busy":"2022-05-01T11:11:57.298747Z","iopub.execute_input":"2022-05-01T11:11:57.299015Z","iopub.status.idle":"2022-05-01T11:11:57.308385Z","shell.execute_reply.started":"2022-05-01T11:11:57.298979Z","shell.execute_reply":"2022-05-01T11:11:57.307648Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Image data format = 'channels_last' indicates Image is in three dimensional array, where third dim represents color channels (RGB)**\n\n1st dim = rows\n\n2nd dim = columns\n\n3rd dim = channels ","metadata":{"id":"gsvvrr624lRM"}},{"cell_type":"code","source":"#write your code here for creating the function. This function should return images & their path\n\n#write your pre-processing steps here (checking only for the first five images here)\nfor img in all_imgs[0:5] :\n    img = tf.io.read_file(img, name=None)\n\n    # we need to decode jpeg encoded images (here by default channels = 0)\n    img = tf.image.decode_jpeg(img, channels=0)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n\n    #append preprocessed images to the list\n    preprocessed_image.append(img)\n","metadata":{"id":"uahCl7VO5GBs","execution":{"iopub.status.busy":"2022-05-01T11:11:57.31125Z","iopub.execute_input":"2022-05-01T11:11:57.312031Z","iopub.status.idle":"2022-05-01T11:11:59.832524Z","shell.execute_reply.started":"2022-05-01T11:11:57.311991Z","shell.execute_reply":"2022-05-01T11:11:59.831662Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking first five images post preprocessing\n\nDisplay_Images = preprocessed_image[0:5]\nfigure, axes = plt.subplots(1,5)\nfigure.set_figwidth(25)\n\nfor ax, image in zip(axes, Display_Images) :\n  print('Shape after resize : ', image.shape)\n  ax.imshow(image)\n  ax.grid('off')\n","metadata":{"id":"RMl1nnpL5GBs","outputId":"12597430-d46d-4fa4-e9cd-c210c69e37b9","execution":{"iopub.status.busy":"2022-05-01T11:11:59.833889Z","iopub.execute_input":"2022-05-01T11:11:59.834295Z","iopub.status.idle":"2022-05-01T11:12:00.709279Z","shell.execute_reply.started":"2022-05-01T11:11:59.834256Z","shell.execute_reply":"2022-05-01T11:12:00.708525Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<a id=\"5\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Dataset Creation</p>\n\n1.Create a function which maps the image path to their feature.\n\n2.Create a builder function to create train & test dataset & apply the function created earlier to transform the dataset\n\n3.Apply train_test_split on both image path & captions to create the train & test list. Create the train-test spliit using 80-20 ratio & random state = 42\n\n2.Make sure you have done Shuffle and batch while building the dataset\n\n3.The shape of each image in the dataset after building should be (batch_size, 8*8, 2048)\n\n4.The shape of each caption in the dataset after building should be(batch_size, max_len)","metadata":{"id":"9Sr-5vXXyPLT"}},{"cell_type":"markdown","source":"**We will have a function here for preprocessing and returing images, so that we can use it for vectorization and preprocesses images parallelly**","metadata":{"id":"VEi3otD5iudP"}},{"cell_type":"code","source":"## write your code here for applying the function to the image path dataset,\n## such that the transformed dataset should contain images & their path\n\n\ndef load_images(image_path) :\n  img = tf.io.read_file(image_path, name = None)\n  img = tf.image.decode_jpeg(img, channels=0)\n  img = tf.image.resize(img, IMAGE_SHAPE)\n  img = tf.keras.applications.inception_v3.preprocess_input(img)\n  return img, image_path","metadata":{"id":"3rOriJpNit1y","execution":{"iopub.status.busy":"2022-05-01T11:12:00.710557Z","iopub.execute_input":"2022-05-01T11:12:00.710988Z","iopub.status.idle":"2022-05-01T11:12:00.716529Z","shell.execute_reply.started":"2022-05-01T11:12:00.710954Z","shell.execute_reply":"2022-05-01T11:12:00.715812Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_img_vector","metadata":{"id":"q0yaT10Wtl1G","outputId":"3fb19489-e49d-4a77-a8ff-8b52675c5d8e","execution":{"iopub.status.busy":"2022-05-01T11:12:00.717961Z","iopub.execute_input":"2022-05-01T11:12:00.718426Z","iopub.status.idle":"2022-05-01T11:12:00.747316Z","shell.execute_reply.started":"2022-05-01T11:12:00.718387Z","shell.execute_reply":"2022-05-01T11:12:00.746537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map each image full path to the function, in order to preprocess the image\n\n## sort the unique paths and store in a list\ntraining_list = sorted(set(all_img_vector))\n\n#create a new dataset from above training list\nNew_Img = tf.data.Dataset.from_tensor_slices(training_list)\n\n#map load_images function across the elements of the new dataset above\nNew_Img = New_Img.map(load_images, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n#Note  : Here, num_parallel_calls = tf.data.experimental.AUTOTUNE sets the number of parallel calls dynamically.\n            ## based on the current GPU/CPU\n\n#setting a batch size of 64\nNew_Img = New_Img.batch(64, drop_remainder=False)\n#Note : As we don't want to drop the last batch if it contains less than 64 elements, we set drop_remainder to false","metadata":{"id":"fJEa5_4Mtl4c","execution":{"iopub.status.busy":"2022-05-01T11:12:00.74875Z","iopub.execute_input":"2022-05-01T11:12:00.74901Z","iopub.status.idle":"2022-05-01T11:12:00.886562Z","shell.execute_reply.started":"2022-05-01T11:12:00.748974Z","shell.execute_reply":"2022-05-01T11:12:00.885922Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note :** As we didn't want to drop the last batch if it contains less than 64 elements, we **set drop_remainder to false**","metadata":{"id":"cAFpIjtOoqWu"}},{"cell_type":"code","source":"New_Img","metadata":{"id":"MIkgxXo3tl76","outputId":"a6a02645-043b-4917-b2f5-a205c25c54e4","execution":{"iopub.status.busy":"2022-05-01T11:12:00.887743Z","iopub.execute_input":"2022-05-01T11:12:00.887968Z","iopub.status.idle":"2022-05-01T11:12:00.894303Z","shell.execute_reply.started":"2022-05-01T11:12:00.887936Z","shell.execute_reply":"2022-05-01T11:12:00.893565Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train-test split\n\n    Split ratio = 80 : 20\n\n    Rabdom state = 42","metadata":{"id":"3wHHaBJgv_AZ"}},{"cell_type":"code","source":"#Ratio = 80:20 and we will set random state = 42\n\npath_train, path_test, caption_train, caption_test = train_test_split(all_img_vector, cap_vector, test_size = 0.2, random_state = 42)","metadata":{"id":"HJDx-qygtl--","execution":{"iopub.status.busy":"2022-05-01T11:12:00.895779Z","iopub.execute_input":"2022-05-01T11:12:00.896305Z","iopub.status.idle":"2022-05-01T11:12:00.921124Z","shell.execute_reply.started":"2022-05-01T11:12:00.896264Z","shell.execute_reply":"2022-05-01T11:12:00.920465Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training data for images: \" + str(len(path_train)))\nprint(\"Testing data for images: \" + str(len(path_test)))\nprint(\"Training data for Captions: \" + str(len(caption_train)))\nprint(\"Testing data for Captions: \" + str(len(caption_test)))","metadata":{"id":"K6VN5SidtmCc","outputId":"deffd957-c38f-4b2e-f573-cbe59cae2641","execution":{"iopub.status.busy":"2022-05-01T11:12:00.922431Z","iopub.execute_input":"2022-05-01T11:12:00.922711Z","iopub.status.idle":"2022-05-01T11:12:00.931638Z","shell.execute_reply.started":"2022-05-01T11:12:00.922647Z","shell.execute_reply":"2022-05-01T11:12:00.93064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load the pretrained Imagenet weights of Inception net V3\n\n\n    1.To save the memory(RAM) from getting exhausted, extract the features of the images using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n\n    2.The shape of the output of this layer is 8x8x2048. \n\n    3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n\n","metadata":{"id":"b3aSUUjB5GBt"}},{"cell_type":"code","source":"image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n\nnew_input = image_model.input #write code here to get the input of the image_model\nhidden_layer = image_model.layers[-1].output  #write code here to get the output of the image_model\n\n#build the final model using both input & output layer\nimage_features_extract_model = tf.compat.v1.keras.Model(new_input, hidden_layer)","metadata":{"id":"nQyVXhKd5GBt","outputId":"f0f50be9-069f-40ca-8712-a979fc383336","execution":{"iopub.status.busy":"2022-05-01T11:12:00.933147Z","iopub.execute_input":"2022-05-01T11:12:00.933506Z","iopub.status.idle":"2022-05-01T11:12:03.733331Z","shell.execute_reply.started":"2022-05-01T11:12:00.933468Z","shell.execute_reply":"2022-05-01T11:12:03.732608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# write the code to apply the feature_extraction model to your earlier created dataset which contained images & their respective paths\n# Once the features are created, you need to reshape them such that feature shape is in order of (batch_size, 8*8, 2048)\n\nimage_features_extract_model.summary()","metadata":{"id":"MQjWdIl25GBt","outputId":"5188653a-fa56-4a60-ea51-d8e1ae7910a4","execution":{"iopub.status.busy":"2022-05-01T11:12:03.734545Z","iopub.execute_input":"2022-05-01T11:12:03.734801Z","iopub.status.idle":"2022-05-01T11:12:03.869864Z","shell.execute_reply.started":"2022-05-01T11:12:03.734768Z","shell.execute_reply":"2022-05-01T11:12:03.86919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### NOTE : As we are not classifying the images here, there is no need to have the softmax layer at the end.","metadata":{"id":"3QftgZmoKMI3"}},{"cell_type":"markdown","source":"### FAQs on how to load the features:\n\n* You can load the features using a dictionary created OR\n* You can store using numpy(np.load) to load the feature vector.","metadata":{"id":"l_R1rvpIoqWy"}},{"cell_type":"code","source":"# extract features from each image in the dataset\n\nimg_features = {}\nfor image, image_path in tqdm(New_Img) :\n  # we are using tqdm for progress bar\n\n  # feed images from newly created Dataset above to Inception V3 built above\n  batch_features = image_features_extract_model(image)\n  #squeeze out the features in a batch\n  batch_features_flattened = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for batch_feat, path in zip(batch_features_flattened, image_path) :\n    feature_path = path.numpy().decode('utf-8')\n    img_features[feature_path] = batch_feat.numpy()","metadata":{"id":"cg8Xvr-BKGZN","outputId":"628b075f-0ba6-42d7-845b-5bcc36b5c1f5","execution":{"iopub.status.busy":"2022-05-01T11:12:03.871629Z","iopub.execute_input":"2022-05-01T11:12:03.871962Z","iopub.status.idle":"2022-05-01T11:13:25.840552Z","shell.execute_reply.started":"2022-05-01T11:12:03.871925Z","shell.execute_reply":"2022-05-01T11:13:25.839708Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_features","metadata":{"id":"yU8yWjlDKGib","outputId":"9c5ddfed-fe19-4835-d4c9-75780a239ad8","execution":{"iopub.status.busy":"2022-05-01T11:13:25.842218Z","iopub.execute_input":"2022-05-01T11:13:25.842486Z","iopub.status.idle":"2022-05-01T11:13:25.874158Z","shell.execute_reply.started":"2022-05-01T11:13:25.84245Z","shell.execute_reply":"2022-05-01T11:13:25.873472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_features_flattened","metadata":{"id":"ev8QoOS5KHRP","outputId":"329a0311-f52f-4a44-f9e7-73e24fd3f50c","execution":{"iopub.status.busy":"2022-05-01T11:13:25.875201Z","iopub.execute_input":"2022-05-01T11:13:25.876035Z","iopub.status.idle":"2022-05-01T11:13:25.896112Z","shell.execute_reply.started":"2022-05-01T11:13:25.875996Z","shell.execute_reply":"2022-05-01T11:13:25.895442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(img_features)","metadata":{"id":"kb1mYVNuKHXz","outputId":"54755a4c-1a22-4572-90d8-8d86eefc74ad","execution":{"iopub.status.busy":"2022-05-01T11:13:25.897228Z","iopub.execute_input":"2022-05-01T11:13:25.897448Z","iopub.status.idle":"2022-05-01T11:13:25.902813Z","shell.execute_reply.started":"2022-05-01T11:13:25.897418Z","shell.execute_reply":"2022-05-01T11:13:25.901922Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_feat.shape","metadata":{"id":"pYb3DI_SN1Ta","outputId":"9feb71af-58a8-48d7-b14a-4fa0cf535f86","execution":{"iopub.status.busy":"2022-05-01T11:13:25.904322Z","iopub.execute_input":"2022-05-01T11:13:25.904824Z","iopub.status.idle":"2022-05-01T11:13:25.911448Z","shell.execute_reply.started":"2022-05-01T11:13:25.904786Z","shell.execute_reply":"2022-05-01T11:13:25.91046Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#view top five items of img_features dict\nimport more_itertools\ntop_5 = more_itertools.take(5, img_features.items())\n\ntop_5","metadata":{"id":"RSRZzSL4N1jN","outputId":"3db0273d-b543-4b16-ce29-6d61fed6d804","execution":{"iopub.status.busy":"2022-05-01T11:13:25.912902Z","iopub.execute_input":"2022-05-01T11:13:25.913386Z","iopub.status.idle":"2022-05-01T11:13:25.931262Z","shell.execute_reply.started":"2022-05-01T11:13:25.913346Z","shell.execute_reply":"2022-05-01T11:13:25.930588Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### FAQs on how to store the features:\n* You can store the features using a dictionary with the path as the key and values as the feature extracted by the inception net v3 model OR\n* You can store using numpy(np.save) to store the resulting vector.","metadata":{"id":"rBgPrQTz5GBu"}},{"cell_type":"markdown","source":"#### We will now feed both, the images as well as  the captions to the model","metadata":{"id":"T86eGmjr5GBv"}},{"cell_type":"code","source":"#to provide, both images along with the captions as input\ndef map(image_name, caption):\n    \n    # your code goes here to create the dataset & transform it\n    \n    img_tensor = img_features[image_name.decode('utf-8')]\n    return img_tensor, caption","metadata":{"id":"Sa1oxIaD1-3G","execution":{"iopub.status.busy":"2022-05-01T11:13:25.932236Z","iopub.execute_input":"2022-05-01T11:13:25.932483Z","iopub.status.idle":"2022-05-01T11:13:25.937013Z","shell.execute_reply.started":"2022-05-01T11:13:25.93245Z","shell.execute_reply":"2022-05-01T11:13:25.936264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### We will set the bufffer size to 1000 and batch size to 64\n\n    In Tensorflow, data is designed to work with huge sequences. Thus instead of shuffling entire sequence, it maintains separate buffer within which it shuffles elements. This buffer size of 1000 refers to the same.\n    \n    \n    We will set reshuffle_each_iteration to True in order to ensure different order per epoch\n    \n    We will also use the prefetch function to prepare later elements as the current ones are being processed\n","metadata":{"id":"c1NIQDY3oqW2"}},{"cell_type":"code","source":"# create a builder function to create dataset which takes in the image path & captions as input\n# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n\nBUFFER_SIZE = 1000\nBATCH_SIZE = 64\ndef gen_dataset(img, capt):\n    \n    # your code goes here to create the dataset & transform it\n    \n    data = tf.data.Dataset.from_tensor_slices((img, capt))\n    # dataset created using tf.data.Dataset.from_tensor_slices\n    data = data.map(lambda ele1, ele2 : tf.numpy_function(map, [ele1, ele2], [tf.float32, tf.int32]),\n                    num_parallel_calls = tf.data.experimental.AUTOTUNE)\n    \n     \n    data = (data.shuffle(BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE, drop_remainder = False)\n    .prefetch(tf.data.experimental.AUTOTUNE))\n    # .prefetch() is used to prepare all upcoming elements, while current elements are being processed\n    # We set reshuffle_each_iteration set to True in order to ensure different order per epoch\n    # Also,  drop_remainder is set to False as we don't want to miss out any element if the last batch contains less than 64 elements\n\n    return data","metadata":{"id":"gglO9Zqh5GBw","scrolled":true,"execution":{"iopub.status.busy":"2022-05-01T11:13:25.938167Z","iopub.execute_input":"2022-05-01T11:13:25.939095Z","iopub.status.idle":"2022-05-01T11:13:25.94738Z","shell.execute_reply.started":"2022-05-01T11:13:25.939055Z","shell.execute_reply":"2022-05-01T11:13:25.946663Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = gen_dataset(path_train,caption_train)\ntest_dataset = gen_dataset(path_test,caption_test)","metadata":{"id":"7AoRRJlL5GBy","execution":{"iopub.status.busy":"2022-05-01T11:13:25.950233Z","iopub.execute_input":"2022-05-01T11:13:25.950436Z","iopub.status.idle":"2022-05-01T11:13:26.130145Z","shell.execute_reply.started":"2022-05-01T11:13:25.950413Z","shell.execute_reply":"2022-05-01T11:13:26.129467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_img_batch, sample_cap_batch = next(iter(train_dataset))\nprint(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\nprint(sample_cap_batch.shape) #(batch_size,max_len)","metadata":{"id":"BfdQNzQl5GBz","outputId":"cad71c70-c3b5-44f5-99e9-2d018fc3f98c","execution":{"iopub.status.busy":"2022-05-01T11:13:26.131369Z","iopub.execute_input":"2022-05-01T11:13:26.131864Z","iopub.status.idle":"2022-05-01T11:13:26.642174Z","shell.execute_reply.started":"2022-05-01T11:13:26.131827Z","shell.execute_reply":"2022-05-01T11:13:26.641363Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Model Building</p>\n\n    1.Set the parameters\n\n    2.Build the Encoder\n\n    3. Build the Attention Model\n\n    4. Build the Decoder","metadata":{"id":"itG9dCfI5GBz"}},{"cell_type":"code","source":"# Setting  parameters\n\nembedding_dim = 256 \nunits = 512\n\n#top 5,000 words +1\nvocab_size = 5001\ntrain_num_steps = len(path_train) // BATCH_SIZE #len(total train images) // BATCH_SIZE\ntest_num_steps = len(path_test) // BATCH_SIZE  #len(total test images) // BATCH_SIZE\n\nmax_length = 31\nfeature_shape = batch_feat.shape[1]\nattention_feature_shape = batch_feat.shape[0]","metadata":{"id":"AIcikdUI5GB0","execution":{"iopub.status.busy":"2022-05-01T11:13:26.643362Z","iopub.execute_input":"2022-05-01T11:13:26.643642Z","iopub.status.idle":"2022-05-01T11:13:26.649876Z","shell.execute_reply.started":"2022-05-01T11:13:26.643604Z","shell.execute_reply":"2022-05-01T11:13:26.648954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encoder (CNN)\n\n    1. The encoder parts involve the convolution of the input image with the help of various convolution, max pooling, and fully connected layers. \n\n    2. Since we are not dealing with the classification of the image, we have removed them from the end. \n\n    3. The final output of the encoder part will be the generation of the feature vector.","metadata":{"id":"H_xPieLr5GB1"}},{"cell_type":"code","source":"tf.compat.v1.reset_default_graph()\nprint(tf.compat.v1.get_default_graph())","metadata":{"id":"30MEu5aesjYz","outputId":"f38ae080-ca05-4637-d8c2-c706b179f244","execution":{"iopub.status.busy":"2022-05-01T11:13:26.651589Z","iopub.execute_input":"2022-05-01T11:13:26.651874Z","iopub.status.idle":"2022-05-01T11:13:26.661017Z","shell.execute_reply.started":"2022-05-01T11:13:26.651837Z","shell.execute_reply":"2022-05-01T11:13:26.659996Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Building Encoder using CNN Keras subclassing method\n\nclass Encoder(Model):\n    def __init__(self,embed_dim):\n        super(Encoder, self).__init__()\n        self.dense = tf.keras.layers.Dense(embed_dim) #build your Dense layer with relu activation\n        \n    def call(self, features):\n        features =  self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n        features =  tf.keras.activations.relu(features, alpha=0.01, max_value=None, threshold=0)\n        return features","metadata":{"id":"Evf8gxEr5GB1","execution":{"iopub.status.busy":"2022-05-01T11:13:26.662769Z","iopub.execute_input":"2022-05-01T11:13:26.663468Z","iopub.status.idle":"2022-05-01T11:13:26.671222Z","shell.execute_reply.started":"2022-05-01T11:13:26.663426Z","shell.execute_reply":"2022-05-01T11:13:26.670252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder=Encoder(embedding_dim)","metadata":{"id":"C68N2xQo5GB2","execution":{"iopub.status.busy":"2022-05-01T11:13:26.672848Z","iopub.execute_input":"2022-05-01T11:13:26.673163Z","iopub.status.idle":"2022-05-01T11:13:26.687492Z","shell.execute_reply.started":"2022-05-01T11:13:26.673126Z","shell.execute_reply":"2022-05-01T11:13:26.6867Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\n#plot_model(encoder, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"id":"TBsI5iVetX7W","execution":{"iopub.status.busy":"2022-05-01T11:13:26.689096Z","iopub.execute_input":"2022-05-01T11:13:26.689402Z","iopub.status.idle":"2022-05-01T11:13:26.695519Z","shell.execute_reply.started":"2022-05-01T11:13:26.689362Z","shell.execute_reply":"2022-05-01T11:13:26.69473Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Note : \n\n > The **CNN-based encoder produces the feature vector which is the encoded representation of the input image.**\n\n > The resulting **feature vector is static** and does not change at each timestamp.\n    \n > Therefore we need to pass this vector to the **attention model along with the hidden state of the decoder to create the context vector**. ","metadata":{}},{"cell_type":"markdown","source":"## Attention model\n\n#### Important Details : \n\n > **Attention is an interface connecting the encoder and decoder that provides the decoder with information from every encoder hidden state.**\n \n > With this framework, the model is able to **selectively focus on valuable parts of the input sequence** and hence, learn the association between them.\n \n > The attention model produces an **output(context vector) that is fed to the decoder** for predicting the word at that timestamp\n \n > This output, i.e **context vector is adaptive in nature and change for each timestamp.**\n \n > It aims to overcome the limitation of traditional CNN-RNN based models. Using this, **instead of passing the complete input image to the RNN at every timestamp, we can pass different relevant parts of the image to it.**\n \n > This makes the model **faster and increase it's prediction accuracy.**","metadata":{"id":"3xwwJdGQ5GB3"}},{"cell_type":"code","source":"class Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units) #build your Dense layer\n        self.W2 = tf.keras.layers.Dense(units) #build your Dense layer\n        self.V = tf.keras.layers.Dense(1) #build your final Dense layer with unit 1\n        self.units=units\n\n    def call(self, features, hidden):\n        # features shape: (batch_size, 8*8, embedding_dim)\n        # hidden shape: (batch_size, hidden_size)\n\n        # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n        hidden_with_time_axis = hidden[:, tf.newaxis]\n\n        # build your score funciton to shape: (batch_size, 8*8, units)\n        score = tf.keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis))  \n\n        # extract your attention weights with shape: (batch_size, 8*8, 1)\n        attention_weights = tf.keras.activations.softmax(self.V(score), axis=1) \n\n        #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n        context_vector = attention_weights * features \n\n        # reduce the shape to (batch_size, embedding_dim)\n        context_vector = tf.reduce_sum(context_vector, axis=1)  \n        \n        return context_vector, attention_weights","metadata":{"id":"bee_QVDs5GB3","execution":{"iopub.status.busy":"2022-05-01T11:13:26.69685Z","iopub.execute_input":"2022-05-01T11:13:26.697127Z","iopub.status.idle":"2022-05-01T11:13:26.707265Z","shell.execute_reply.started":"2022-05-01T11:13:26.697086Z","shell.execute_reply":"2022-05-01T11:13:26.705878Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Note \n\n    1. Attention model does a linear transformation of input. Hence we have aplied tanh on top of it, in order to introduce non-linearities and achieve a smoother distribution.\n    \n    2. The features produced from fully connected dense layer of encoder and the hidden state of decoder are fed to tanh function and attention score is calculated\n    \n    3. As we require an output in range (0,1), we will apply softmax function to attention score and get the final attention weights","metadata":{"id":"QqGCs_CBoqW_"}},{"cell_type":"markdown","source":"## Decoder\n\n    RNN - Model (GRU)\n\n > **Input to GRU : Context vector (from attention model) concatenated with  embedded vector (embedding layer)**. Output of this concatenation layer is fed to GRU as input\n \n  > **Embedding layer present inside the decoder takes the input sequence** (preprocessed and transformed such that all samples have equal sequence length - through padding followed by masking). Embedding layer transforms this into an embedded vector.\n  \n  > **Concatenation layer contains : Embedded vector (output of embedding layer) along with the Context vector** (output of attention model)","metadata":{"id":"sE2aLpaM5GB4"}},{"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units\n        self.attention = Attention_model(self.units) #iniitalise your Attention model with units\n        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim) #build your Embedding layer\n        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n        \n\n    def call(self,x,features, hidden):\n        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model\n        embed = self.embed(x) # embed your input to shape: (batch_size, 1, embedding_dim)\n        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis = -1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n        output,state = self.gru(embed) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n        output = self.d1(output)\n        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n        \n        return output, state, attention_weights\n    \n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","metadata":{"id":"ol38QKyR5GB4","execution":{"iopub.status.busy":"2022-05-01T11:13:26.708919Z","iopub.execute_input":"2022-05-01T11:13:26.709494Z","iopub.status.idle":"2022-05-01T11:13:26.721761Z","shell.execute_reply.started":"2022-05-01T11:13:26.709439Z","shell.execute_reply":"2022-05-01T11:13:26.721056Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder=Decoder(embedding_dim, units, vocab_size)","metadata":{"id":"2H-Qegu05GB5","execution":{"iopub.status.busy":"2022-05-01T11:13:26.723377Z","iopub.execute_input":"2022-05-01T11:13:26.723703Z","iopub.status.idle":"2022-05-01T11:13:26.748256Z","shell.execute_reply.started":"2022-05-01T11:13:26.723643Z","shell.execute_reply":"2022-05-01T11:13:26.747544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features=encoder(sample_img_batch)\n\nhidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n\npredictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\nprint('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\nprint('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\nprint('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)","metadata":{"id":"CYg60cuC5GB5","outputId":"a2faae6c-be6e-4cdf-cf29-f1d969426d5f","execution":{"iopub.status.busy":"2022-05-01T11:13:26.749615Z","iopub.execute_input":"2022-05-01T11:13:26.749896Z","iopub.status.idle":"2022-05-01T11:13:27.027578Z","shell.execute_reply.started":"2022-05-01T11:13:26.74986Z","shell.execute_reply":"2022-05-01T11:13:27.026842Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Model Training and Evaluation</p>\n1.Set the optimizer & loss object\n\n2.Create your checkpoint path\n\n3.Create your training & testing step functions\n\n4.Create your loss function for the test dataset","metadata":{"id":"azD6dvdS5GB6"}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)  #define the optimizer\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE) #define your loss object","metadata":{"id":"79nJFjv05GB6","execution":{"iopub.status.busy":"2022-05-01T11:13:27.028999Z","iopub.execute_input":"2022-05-01T11:13:27.029252Z","iopub.status.idle":"2022-05-01T11:13:27.034746Z","shell.execute_reply.started":"2022-05-01T11:13:27.029217Z","shell.execute_reply":"2022-05-01T11:13:27.033078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    #loss is getting multiplied with mask to get an ideal shape\n    \n    return tf.reduce_mean(loss_)","metadata":{"id":"2F2niqM35GCG","execution":{"iopub.status.busy":"2022-05-01T11:13:27.036379Z","iopub.execute_input":"2022-05-01T11:13:27.036695Z","iopub.status.idle":"2022-05-01T11:13:27.044597Z","shell.execute_reply.started":"2022-05-01T11:13:27.036639Z","shell.execute_reply":"2022-05-01T11:13:27.043658Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Why Masking ?\n\n *Padding can result in a risk of adding penalty to the model.* \n\n  *Once the padding is done, we need to apply 'masking'.*\n    \n > Without masking, the model will **consider the padded input at that timestep, which will contribute to an increased loss.**\n    \n > Through masking we need to inform the model to **ignore whenever a padded input is passed at a timestep**, hinting that this part of the input is padded. \n \n\n\n*We apply masking to rectify the same and this will truncate down all the added penalties back to zero :*\n\n    \n### Loss  = Loss * Mask\n","metadata":{}},{"cell_type":"code","source":"checkpoint_path = \"Flickr8K/checkpoint1\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","metadata":{"id":"UiQUd7305GCI","execution":{"iopub.status.busy":"2022-05-01T11:13:27.046361Z","iopub.execute_input":"2022-05-01T11:13:27.04668Z","iopub.status.idle":"2022-05-01T11:13:27.055046Z","shell.execute_reply.started":"2022-05-01T11:13:27.046626Z","shell.execute_reply":"2022-05-01T11:13:27.054206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])","metadata":{"id":"bZ6yPbrG5GCJ","execution":{"iopub.status.busy":"2022-05-01T11:13:27.05629Z","iopub.execute_input":"2022-05-01T11:13:27.056644Z","iopub.status.idle":"2022-05-01T11:13:27.065106Z","shell.execute_reply.started":"2022-05-01T11:13:27.056603Z","shell.execute_reply":"2022-05-01T11:13:27.0642Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* While creating the training step for your model, you will apply Teacher forcing.\n\n\n### Why Teacher Forcing ?\n\nThere are multiple issues with with training recurrent neural networks that use output from prior time steps as input :\n\n    Slow convergence\n    \n    Model instability\n    \n    Poor skill\n\nIf the **previous output is incorrect (by any chance), it will result in inaccurate input for the next time stamp** which will further result in a different output than expected and the process will continue. \n\n**As a result, the model will get off track and will get punished for every subsequent word it generates. This makes learning slower and the model unstable.**\n\n*To address this we  use Teacher Forcing*\n\n > Teacher forcing is a fast and effective way to train a recurrent neural network, where the **target/real word (i.e ground truth) is passed as the next input to the decoder instead of previous prediciton or output.**\n \n \n > Training with Teacher Forcing **converges faster.** At the early stages of training, the predictions of the model are very bad.\n \n > If we do not use Teacher Forcing, the hidden states of the model will be updated by a sequence of wrong predictions, errors will accumulate, and it is difficult for the model to learn from that.","metadata":{"id":"gd0le5Un5GCJ"}},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    \n    with tf.GradientTape() as tape:\n        #write your code here to do the training steps\n        encoder_op = encoder(img_tensor)\n\n        #apply teacher forcing by passing target word as next input to the decoder\n        for r in range(1, target.shape[1]) :\n          #pass encoder_op to decoder\n          predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n          loss = loss + loss_function(target[:, r], predictions) \n          dec_input = tf.expand_dims(target[:, r], 1)  \n\n    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n    grad = tape.gradient (loss, trainable_vars) # calculating gradient wrt each trainable var\n\n    #we will now compute the gradients and apply it to the optimizer while backpropagating\n    optimizer.apply_gradients(zip(grad, trainable_vars))\n\n    return loss, avg_loss","metadata":{"id":"Q2pig7yS5GCK","execution":{"iopub.status.busy":"2022-05-01T11:13:27.066769Z","iopub.execute_input":"2022-05-01T11:13:27.067126Z","iopub.status.idle":"2022-05-01T11:13:27.076587Z","shell.execute_reply.started":"2022-05-01T11:13:27.067088Z","shell.execute_reply":"2022-05-01T11:13:27.075578Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* While creating the test step for your model, you will pass your previous prediciton as the next input to the decoder.","metadata":{"id":"jFz-UvRo5GCK"}},{"cell_type":"code","source":"@tf.function\ndef test_step(img_tensor, target):\n    loss = 0\n    \n    #write your code here to do the testing steps\n    hidden = decoder.init_state(batch_size = target.shape[0])\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    with tf.GradientTape() as tape:\n      encoder_op = encoder(img_tensor)\n\n      #apply teacher forcing again\n      for r in range(1, target.shape[1]) :\n        #pass encoder_op to decoder\n        predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n        loss = loss + loss_function(target[:, r], predictions)\n\n        dec_input = tf.expand_dims(target[: , r], 1)\n\n    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n    grad = tape.gradient (loss, trainable_vars) # calculating gradient wrt each trainable var\n\n    #we will now compute the gradients and apply it to the optimizer while backpropagating\n    optimizer.apply_gradients(zip(grad, trainable_vars))                      \n\n\n    return loss, avg_loss","metadata":{"id":"-CDnlwOH5GCL","execution":{"iopub.status.busy":"2022-05-01T11:13:27.077891Z","iopub.execute_input":"2022-05-01T11:13:27.078816Z","iopub.status.idle":"2022-05-01T11:13:27.090402Z","shell.execute_reply.started":"2022-05-01T11:13:27.078776Z","shell.execute_reply":"2022-05-01T11:13:27.089441Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_loss_cal(test_dataset):\n    total_loss = 0\n\n    #write your code to get the average loss result on your test data\n    for (batch, (img_tensor, target)) in enumerate(test_dataset) :\n      batch_loss, t_loss = test_step(img_tensor, target)\n      total_loss = total_loss + t_loss\n      avg_test_loss = total_loss/ test_num_steps\n\n    return avg_test_loss","metadata":{"id":"MY25F3fo5GCL","execution":{"iopub.status.busy":"2022-05-01T11:13:27.091909Z","iopub.execute_input":"2022-05-01T11:13:27.092274Z","iopub.status.idle":"2022-05-01T11:13:27.103177Z","shell.execute_reply.started":"2022-05-01T11:13:27.092234Z","shell.execute_reply":"2022-05-01T11:13:27.102299Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_plot = []\ntest_loss_plot = []\nEPOCHS = 15\n\nbest_test_loss=100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n        avg_train_loss=total_loss / train_num_steps\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n        best_test_loss = test_loss\n        ckpt_manager.save()","metadata":{"id":"j5LGvDQ15GCL","outputId":"d30413a3-8964-4ec6-a3e5-3029a4ecf8ee","execution":{"iopub.status.busy":"2022-05-01T11:13:27.104869Z","iopub.execute_input":"2022-05-01T11:13:27.10515Z","iopub.status.idle":"2022-05-01T11:32:40.173918Z","shell.execute_reply.started":"2022-05-01T11:13:27.105113Z","shell.execute_reply":"2022-05-01T11:32:40.173144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final Test loss after 15 epochs = 0.482\n\n*Let us now plot the training vs test loss and check the trend*","metadata":{"id":"dO753jtVoqXG"}},{"cell_type":"code","source":"from matplotlib.pyplot import figure\nfigure(figsize=(12, 8))\nplt.plot(loss_plot, color='orange', label = 'training_loss_plot')\nplt.plot(test_loss_plot, color='green', label = 'test_loss_plot')\nplt.xlabel('Epochs', fontsize = 15, color = 'red')\nplt.ylabel('Loss', fontsize = 15, color = 'red')\nplt.title('Loss Plot', fontsize = 20, color = 'red')\nplt.legend()\nplt.show()\n","metadata":{"id":"uVVSCYfk5GCM","outputId":"77904d58-15a8-443f-dff9-4fdea1e7dc2f","execution":{"iopub.status.busy":"2022-05-01T11:32:40.190537Z","iopub.execute_input":"2022-05-01T11:32:40.190849Z","iopub.status.idle":"2022-05-01T11:32:40.406152Z","shell.execute_reply.started":"2022-05-01T11:32:40.190819Z","shell.execute_reply":"2022-05-01T11:32:40.405504Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" > **Test Loss : Indicated by the green line, shows a steady pattern along with training loss (indicated by orange), despite the use of teacher forcing**\n\n > Also, we will **stick to 15 approaches and not increase them further, as the intent here is not to create the state of art model. Rather understand how to integrate attention mechanism with E-D architecture for images.**","metadata":{"id":"X10QTtAioqXG"}},{"cell_type":"markdown","source":"### NOTE : \n\n* Since there is a difference between the train & test steps (Presence of teacher forcing), you may observe that the train loss is decreasing while your test loss is not. \n\n* This doesn't mean that the model is overfitting, as we can't compare the train & test results here, as both approach is different.\n","metadata":{"id":"04dvD81C5GCM"}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Final Model Evaluation</p>\n1.Define your evaluation function using greedy search\n\n2.Define your evaluation function using beam search ( optional)\n\n3.Test it on a sample data using BLEU score","metadata":{"id":"oDjEZjWK5GCM"}},{"cell_type":"markdown","source":"### Greedy Search\n\n > This method is a simple approximation technique which calculates the **probability of the words according to their occurrence in the English vocabulary.**\n \n > It **takes the sample of the words, finds the probability of each of the words, and then outputs the word with the highest probability.**\n \n > Greedy Search will always consider **only one best alternative and this makes the computational speed of the model fast**, but the accuracy might not be up to the mark.","metadata":{"id":"ZuVq217H5GCN"}},{"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_feature_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_images(image)[0], 0) #process the input image to desired format before extracting features\n    img_tensor_val = image_features_extract_model(temp_input) # Extract features using our feature extraction model\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder (img_tensor_val) # extract the features by passing the input to encoder\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden) # get the output from decoder\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy() #extract the predicted id(embedded value) which carries the max value\n        #map the id to the word from tokenizer and append the value to the result list\n        result.append (tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot,predictions\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot,predictions\n","metadata":{"id":"sKBTxbtT5GCN","execution":{"iopub.status.busy":"2022-05-01T11:32:40.407302Z","iopub.execute_input":"2022-05-01T11:32:40.407712Z","iopub.status.idle":"2022-05-01T11:32:40.4177Z","shell.execute_reply.started":"2022-05-01T11:32:40.407661Z","shell.execute_reply":"2022-05-01T11:32:40.417003Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_attention_map (caption, weights, image) :\n\n  fig = plt.figure(figsize = (10, 10))\n  temp_img = np.array(Image.open(image))\n\n  cap_len = len(caption)\n  for cap in range(cap_len) :\n    weights_img = np.reshape(weights[cap], (8,8))\n    wweights_img = np.array(Image.fromarray(weights_img).resize((224,224), Image.LANCZOS))\n\n    ax = fig.add_subplot(cap_len//2, cap_len//2, cap+1)\n    ax.set_title(caption[cap], fontsize = 14, color = 'red')\n\n    img = ax.imshow(temp_img)\n\n    ax.imshow(weights_img, cmap='gist_heat', alpha=0.6, extent=img.get_extent())\n    ax.axis('off')\n  plt.subplots_adjust(hspace=0.2, wspace=0.2)\n  plt.show()","metadata":{"id":"X35rme2SlanC","execution":{"iopub.status.busy":"2022-05-01T11:32:40.419692Z","iopub.execute_input":"2022-05-01T11:32:40.419935Z","iopub.status.idle":"2022-05-01T11:32:40.431285Z","shell.execute_reply.started":"2022-05-01T11:32:40.419909Z","shell.execute_reply":"2022-05-01T11:32:40.430597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu","metadata":{"id":"W04Ya3Pk5GCO","execution":{"iopub.status.busy":"2022-05-01T11:32:40.43254Z","iopub.execute_input":"2022-05-01T11:32:40.432837Z","iopub.status.idle":"2022-05-01T11:32:40.871239Z","shell.execute_reply.started":"2022-05-01T11:32:40.432803Z","shell.execute_reply":"2022-05-01T11:32:40.870496Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filt_text(text):\n    filt=['<start>','<unk>','<end>'] \n    temp= text.split()\n    [temp.remove(j) for k in filt for j in temp if k==j]\n    text=' '.join(temp)\n    return text","metadata":{"id":"ptbJAVg15GCP","execution":{"iopub.status.busy":"2022-05-01T11:32:40.874222Z","iopub.execute_input":"2022-05-01T11:32:40.874424Z","iopub.status.idle":"2022-05-01T11:32:40.880075Z","shell.execute_reply.started":"2022-05-01T11:32:40.874398Z","shell.execute_reply":"2022-05-01T11:32:40.879332Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_test = path_test.copy()","metadata":{"id":"l9nVZC-1JvAc","execution":{"iopub.status.busy":"2022-05-01T11:32:40.881558Z","iopub.execute_input":"2022-05-01T11:32:40.882387Z","iopub.status.idle":"2022-05-01T11:32:40.888852Z","shell.execute_reply.started":"2022-05-01T11:32:40.882345Z","shell.execute_reply":"2022-05-01T11:32:40.888091Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pred_caption_audio(random, autoplay=False, weights=(0.5, 0.5, 0, 0)) :\n\n    cap_test_data = caption_test.copy()\n    rid = np.random.randint(0, random)\n    test_image = image_test[rid]\n    #test_image = './images/413231421_43833a11f5.jpg'\n    #real_caption = '<start> black dog is digging in the snow <end>'\n\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test_data[rid] if i not in [0]])\n    result, attention_plot, pred_test = evaluate(test_image)\n\n\n    real_caption=filt_text(real_caption)      \n\n\n    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n\n\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = pred_caption.split()\n\n    score = sentence_bleu(reference, candidate, weights=weights)#set your weights\n    print(f\"BELU score: {score*100}\")\n    print ('Real Caption:', real_caption)\n    print ('Prediction Caption:', pred_caption)\n    plot_attention_map(result, attention_plot, test_image)\n\n    # we will make use of Google Text to Speech API (online), which will convert the caption to audio\n    speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'en', slow = False)\n    speech.save('voice.mp3')\n    audio_file = 'voice.mp3'\n\n    display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))\n\n    return test_image\n    \n","metadata":{"id":"pMVOLGCi5GCP","execution":{"iopub.status.busy":"2022-05-01T11:32:40.889875Z","iopub.execute_input":"2022-05-01T11:32:40.890124Z","iopub.status.idle":"2022-05-01T11:32:40.9345Z","shell.execute_reply.started":"2022-05-01T11:32:40.890087Z","shell.execute_reply":"2022-05-01T11:32:40.93374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Model Testing</p>\n\nIt is finally time to check how our model performs on Images. \n\n > **Our evaluation metric : BLUE SCORE** (Bilingual Evaluation Understudy)\n \n  ### Note : \n  \n    1.  BLEU is a well-acknowledged metric to measure similarity of one hypothesis sentence to multiple reference sentences. Given a single hypothesis sentence and multiple reference sentences, it returns a value between 0 and 1. \n    \n  **The metric close to 1 means that the two are very similar.**\n    \n    2. We use the BLEU measure to evaluate the result of the test set generated captions. The BLEU is simply taking the fraction of n-grams in the predicted sentence that appears in the ground-truth.","metadata":{"id":"sLXfPAk_nTSz"}},{"cell_type":"markdown","source":"\n# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:100%;text-align:left;border-radius:20px 60px;\">Test Image 1</p>","metadata":{"id":"q-T-mvyYoqXQ"}},{"cell_type":"code","source":"test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.25, 0, 0))\nImage.open(test_image)","metadata":{"id":"d-o4rxHVKU2F","outputId":"309384f9-3d1b-4fd6-d6a6-31ae17cb0094","execution":{"iopub.status.busy":"2022-05-01T11:33:37.121202Z","iopub.execute_input":"2022-05-01T11:33:37.121464Z","iopub.status.idle":"2022-05-01T11:33:38.481076Z","shell.execute_reply.started":"2022-05-01T11:33:37.121433Z","shell.execute_reply":"2022-05-01T11:33:38.47711Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### WOW!! 100% accuracy as indicated by BLEU Score","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:100%;text-align:left;border-radius:20px 60px;\">Test Image 2</p>","metadata":{"id":"s_Mfh4g2oqXR"}},{"cell_type":"code","source":"test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.25, 0, 0))\nImage.open(test_image)","metadata":{"id":"o0JdNHBRLLlz","outputId":"f641c9e9-2682-4dff-cf40-ef927f0d9466","execution":{"iopub.status.busy":"2022-05-01T11:34:07.941455Z","iopub.execute_input":"2022-05-01T11:34:07.941737Z","iopub.status.idle":"2022-05-01T11:34:09.128245Z","shell.execute_reply.started":"2022-05-01T11:34:07.941706Z","shell.execute_reply":"2022-05-01T11:34:09.127526Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:100%;text-align:left;border-radius:20px 60px;\">Test Image 3</p>","metadata":{"id":"zpYcGOF6oqXS"}},{"cell_type":"code","source":"test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.25, 0, 0))\nImage.open(test_image)","metadata":{"id":"X7xo5YY_LQmA","outputId":"1bbb6406-43e7-4cb2-94ef-676ccf8a4e1c","execution":{"iopub.status.busy":"2022-05-01T11:34:23.504086Z","iopub.execute_input":"2022-05-01T11:34:23.504621Z","iopub.status.idle":"2022-05-01T11:34:24.65237Z","shell.execute_reply.started":"2022-05-01T11:34:23.504582Z","shell.execute_reply":"2022-05-01T11:34:24.651729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 100% accuracy yet again! So this combination of weights actually work better than others.","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:100%;text-align:left;border-radius:20px 60px;\">Test Image 4</p>","metadata":{"id":"KZTiflrGoqXT"}},{"cell_type":"code","source":"test_image = pred_caption_audio(len(image_test), True, weights = (0.35, 0.25, 0, 0))\nImage.open(test_image)","metadata":{"id":"CYHX_RtCjifI","outputId":"1dc6f447-43b9-43f0-b6f3-930309f5e41c","execution":{"iopub.status.busy":"2022-05-01T11:34:28.74606Z","iopub.execute_input":"2022-05-01T11:34:28.746751Z","iopub.status.idle":"2022-05-01T11:34:30.021184Z","shell.execute_reply.started":"2022-05-01T11:34:28.746704Z","shell.execute_reply":"2022-05-01T11:34:30.020562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:100%;text-align:left;border-radius:20px 60px;\">Test Image 5</p>","metadata":{"id":"AYYt8YHFoqXU"}},{"cell_type":"code","source":"test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.5, 0, 0))\nImage.open(test_image)","metadata":{"id":"yklC1K0akhdH","outputId":"d7ae1861-62d7-47fb-9ca6-cdd1ac1840fc","execution":{"iopub.status.busy":"2022-05-01T11:34:34.08084Z","iopub.execute_input":"2022-05-01T11:34:34.081121Z","iopub.status.idle":"2022-05-01T11:34:35.327295Z","shell.execute_reply.started":"2022-05-01T11:34:34.081089Z","shell.execute_reply":"2022-05-01T11:34:35.326505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:100%;text-align:left;border-radius:20px 60px;\">Test Image 6</p>","metadata":{"id":"af2z7V1VoqXn"}},{"cell_type":"code","source":"test_image = pred_caption_audio(len(image_test), True, weights = (0.25, 0.5, 0, 0))\nImage.open(test_image)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:31:00.169881Z","iopub.execute_input":"2022-05-01T12:31:00.170628Z","iopub.status.idle":"2022-05-01T12:31:01.270284Z","shell.execute_reply.started":"2022-05-01T12:31:00.170582Z","shell.execute_reply":"2022-05-01T12:31:01.2697Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:100%;text-align:left;border-radius:20px 60px;\">Test Image 7</p>","metadata":{}},{"cell_type":"code","source":"test_image = pred_caption_audio(len(image_test), True, weights = (0.25, 0.25, 0, 0))\nImage.open(test_image)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:32:34.04894Z","iopub.execute_input":"2022-05-01T12:32:34.049517Z","iopub.status.idle":"2022-05-01T12:32:35.099998Z","shell.execute_reply.started":"2022-05-01T12:32:34.049476Z","shell.execute_reply":"2022-05-01T12:32:35.099233Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:seoge print;color:slateblue ;font-size:100%;text-align:left;border-radius:20px 60px;\">Test Image 8</p>","metadata":{}},{"cell_type":"code","source":"test_image = pred_caption_audio(len(image_test), True, weights = (0.25, 0.5, 0, 0))\nImage.open(test_image)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:33:55.472567Z","iopub.execute_input":"2022-05-01T12:33:55.473138Z","iopub.status.idle":"2022-05-01T12:33:56.616141Z","shell.execute_reply.started":"2022-05-01T12:33:55.473097Z","shell.execute_reply":"2022-05-01T12:33:56.615543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<a id=\"9\"></a>\n# <p style=\"background-color:slateblue;font-family:newtimeroman;font-size:150%;color:white;text-align:center;border-radius:20px 60px;\">Conclusion</p>\n","metadata":{}},{"cell_type":"markdown","source":"### 1. We experimented with multiple other weights (12-15 different combinations) but found the best results mainly for the below :\n\n    1. (0.5, 0.5 , 0, 0)\n    \n    2. (0.5, 0.25 , 0, 0)\n    \n    3. (0.25, 0.25 , 0, 0)\n    \n    4. (0.25, 0.35 , 0, 0)\n    \n > The probability of getting a **lower Bleu (less than 50%) score was least for the above weights.**\n \n > We achieved **more than 70% accuracy for majority of the test images using above weights.**\n \n > We also got an **accuracy of 100%  multiple time using weights : (0.5, 0.5 , 0, 0), (0.5, 0.25, 0, 0) and (0.25, 0.5, 0, 0)**\n \n \n ### 2. We used Inception V3 Model with pretrained weights (Imagenet), for CNN (to extract feature vectors) \n \n    1. Inception v3 is a widely-used image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset\n     \n     2. The model is the culmination of many ideas developed by multiple researchers over the years.\n     \n     3. The model itself is made up of symmetric and asymmetric building blocks, including convolutions, average pooling, max pooling, concatinations, dropouts, and fully connected layers. \n     \n     4. Batch normalization is used extensively throughout the model and applied to activation inputs. Loss is computed using Softmax.\n     \n### 3. GRU has been used here for the Decoder (RNN)\n\n    1. Attention Model has also been used in Decoder to ensure higher focus on relevant and particular parts of the image at a given timestamp, rather than focussing on entire image.\n    \n    2. This ensured a higher accuracy, reduced noise and faster computation.\n    \n    3. The output from the decoder - i.e predicted caption and  hidden state, is fed back to the model and predictions were used to calculate the loss.\n    \n    4. We used cross entropy - SparseCategoricalCrossentropy to calculate the loss.\n    \n    5. We also made use of Teacher forcing to decide the next input to the decoder. It ensured faster convergence and a reliably stable model.\n    \n ### 4. Model Training and  Evaluation\n \n     1. We trained the model in 15 EPOCHS and the final loss got reduced to 0.482.\n     \n     2. We chose not to use further epochs as the intent here was not to create the state of art model. Rather understand how to integrate attention mechanism with E-D architecture for images.\n \n \n     3. Prediction of Captions is done through calculating the probabilities of a specific word in the vocabulary.\n     \n     4. We made use of Greedy Search to calculate the probability of words as per their occurrence in the given vocabulary list. It outputs the word with the highest probability.\n     \n     6. We made use of BLUE Score to evaluate our model performance and accuracy. Higher the BLUE score, better the model performance.\n     \n     7. We tested the model with different combinations of weights and achieve the max Bleu Score (100%) for several combinations.\n     \n     \n \n ----------------------------------------------------------","metadata":{"id":"JVj4N4J9oqX4"}},{"cell_type":"markdown","source":"![](https://voximplant.com/assets/images/2020/07/03/speech-to-text.png)","metadata":{}}]}